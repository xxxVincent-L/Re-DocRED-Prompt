{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import collections\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File definition and load\n",
    "test_file = './data/test_revised.json'\n",
    "relation_file = './data/relations.txt'\n",
    "latest_dataset = './data/self-made-dataset.json'\n",
    "docred_file = './data/docred-test.json'\n",
    "\n",
    "with open(test_file, \"rb\") as fh:\n",
    "    data = json.load(fh)\n",
    "\n",
    "with open(latest_dataset, \"rb\") as fhs:\n",
    "    self_made_data = json.load(fhs)\n",
    "with open(docred_file, \"r\") as fh1:\n",
    "    docred_data = json.load(fh1)\n",
    "    \n",
    "    \n",
    "output_file_list = {'basic': './output/basic.txt', 'attention':'./output/attention.txt','batch':'./output/batch.txt','order': './output/order.txt','shuffle':'./output/shuffle.txt'}\n",
    "output_file = output_file_list['order']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def vertex_process(vertex_set):\n",
    "    \n",
    "#     vertex = [item['name'] for item in vertex_set[0]]\n",
    "# #     print(vertex[1])\n",
    "    \n",
    "def relation_triple(labels, vertex_set,):\n",
    "    triples = []\n",
    "    head_entity = []\n",
    "    tail_entity = []\n",
    "    relation_labels = []\n",
    "    evidences = []\n",
    "    for sample in labels:\n",
    "        head_entity.append(vertex_set[sample['h']][0]['name'])\n",
    "        tail_entity.append(vertex_set[sample['t']][0]['name'])\n",
    "        relation_labels.append(rel_dict[sample['r']])\n",
    "        # evidences.append()\n",
    "            \n",
    "    triples = list(zip(head_entity,tail_entity,relation_labels))\n",
    "    \n",
    "    return triples\n",
    "\n",
    "# Entity's Type\n",
    "\n",
    "entity_type_dict = {\n",
    "    'MISC': 'not of a specific type',\n",
    "    'PER': 'person',\n",
    "    'ORG': 'organization',\n",
    "    'LOC': 'location',\n",
    "    'TIME': 'time',\n",
    "    'NUM': 'number'\n",
    "    \n",
    "}\n",
    "\n",
    "def entity_type(vertex_set):\n",
    "    \n",
    "    name_type_dict = collections.defaultdict(int)\n",
    "    for sample in vertex_set:\n",
    "#         name = vertex_set[sample['h']][0]['name']\n",
    "        name = sample[0]['name']\n",
    "        entity_type = sample[0]['type']\n",
    "#         entity_type = vertex_set[sample['h']][0]['type']\n",
    "        original_type = entity_type_dict[entity_type]\n",
    "        \n",
    "        name_type_dict[name] = original_type\n",
    "    \n",
    "    return name_type_dict\n",
    "\n",
    "def relation_combination(triple):\n",
    "\n",
    "    new_triple = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(triple):\n",
    "        merged = False\n",
    "        j = i + 1\n",
    "        while j < len(triple):\n",
    "            if triple[i][0] == triple[j][0] and triple[i][1] == triple[j][1]:\n",
    "                if not merged:\n",
    "                    temp = [triple[i][2], triple[j][2]]\n",
    "                    new_triple.append((triple[i][0], triple[i][1], temp))\n",
    "                    merged = True\n",
    "                triple.pop(j)\n",
    "            else:\n",
    "                j += 1\n",
    "        if not merged:\n",
    "            new_triple.append((triple[i][0], triple[i][1],[triple[i][2]]))\n",
    "        i += 1\n",
    "    return new_triple\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "[(0, 6, 'P577'), (0, 2, 'P175'), (10, 8, 'P131'), (8, 7, 'P17'), (10, 7, 'P17'), (2, 1, 'P27'), (8, 5, 'P30'), (0, 14, 'P577'), (2, 0, 'P800'), (8, 7, 'P131'), (10, 7, 'P131')]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 22\n",
    "rel_dict = collections.defaultdict(int)\n",
    "choosed_documents = [3,7,17,24,28] # after shuffle\n",
    "\n",
    "\n",
    "# Display all the relations. \n",
    "def relations_design(relation_file, rel_dict):\n",
    "    with open(relation_file,'r') as rf:\n",
    "        relations = rf.readlines()\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(relations)\n",
    "    print(\"RELATIONS:==================\")\n",
    "    print(relations)\n",
    "    output_rel_grp = []\n",
    "\n",
    "    output_rel = ''\n",
    "    for i, r in enumerate(relations):\n",
    "        \n",
    "\n",
    "        # Split the lable and relation\n",
    "        label = r.split(' ', 1)[0]\n",
    "        relation = r.split(' ', 1)[1][:-1]\n",
    "        \n",
    "        # Form the dictionary of label and relation\n",
    "        rel_dict[label] = relation\n",
    "        \n",
    "        # Generate the relation for prompt\n",
    "        temp = '- ' + relation +'\\n'\n",
    "\n",
    "        group_size = 30\n",
    "        output_rel +=   temp\n",
    "\n",
    "        if (i+1) % group_size == 0 or i == 95:\n",
    "            output_rel_grp.append(output_rel)    \n",
    "            output_rel = ''\n",
    "\n",
    "            \n",
    "            \n",
    "    return output_rel_grp\n",
    "\n",
    "# rel_in_prompt = relations_design(relation_file, rel_dict)\n",
    "\n",
    "\n",
    "def preprocess(article_data):\n",
    "    \n",
    "    # Read the conponents\n",
    "    \n",
    "    title = article_data['title']\n",
    "    vertex_set = article_data['vertexSet']\n",
    "    sentence = article_data['sents']\n",
    "    labels = article_data['labels']\n",
    "    \n",
    "\n",
    "    # Paragraph Form -> Insight 1: Robustness of sentence.\n",
    "    sentence_list = [' '.join(s) for s in sentence]\n",
    "    paragraph = ' '.join(sentence_list)\n",
    "    words_list = paragraph.split(\" \")\n",
    "#     print(words_list[41], words_list[43])\n",
    "\n",
    "\n",
    "    # Process the relation and generate the triples\n",
    "    \n",
    "    rel_in_prompt = relations_design(relation_file, rel_dict)\n",
    "    triples = relation_combination(relation_triple(labels, vertex_set))\n",
    "    name_type_dict = entity_type(vertex_set)\n",
    "    print(name_type_dict)\n",
    "    print(triples)\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(\"Ground Truth Triples: \\n\")\n",
    "        for i in triples:\n",
    "            f.write(str(i))\n",
    "        f.write(\"\\n\")\n",
    "    return triples, rel_in_prompt, paragraph, name_type_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Different Instructions\n",
    "def instructions_design():\n",
    "    \n",
    "    instruction = collections.defaultdict(int)\n",
    "    \n",
    "    instruction['baseline'] = 'Instruction: the paragraph is from an article of Wikipedia. Category the relation between the entity \"<SEP>\" and entity \"<SEP>\" to one or more available values, the available values are:'\n",
    "    instruction['reformulate'] = 'Instruction: the paragraph is from an article of Wikipedia. Specify the relations that exist between the entity \"<SEP>\" and entity \"<SEP>\" . '\n",
    "    instruction['attention'] = 'Attention: You are restricted to choose from the following relationships and if there are matching relationships below, please output the relations directly, otherwise output \"NONE\":\\n'\n",
    "    instruction['orderOfEntity'] = 'Instruction: the paragraph is from an article of Wikipedia.<SEP> For entity \"<SEP>\", what relationship is entity \"<SEP>\" to it?'\n",
    "    instruction['orderOfParagraph'] = 'Instruction: the paragraph is from an article of Wikipedia.<SEP> Specify the relations that exist between the entity \"<SEP>\" and entity \"<SEP>\" based on the paragraph.'\n",
    "    instruction['entityType'] = 'The type of the entity \"<SEP>\" is \"<SEP>\", and the type of the entity \"<SEP>\" is \"<SEP>\". '\n",
    "    instruction['evidence'] = 'The two entities show in the sentence: [<SEP>]'\n",
    "    \n",
    "    return instruction\n",
    "\n",
    "def prompt_design(head_entity,tail_entity, rel_in_prompt, paragraph, name_type_dict):\n",
    "    \n",
    "    head_type = name_type_dict[head_entity]\n",
    "    tail_type = name_type_dict[tail_entity]\n",
    "    \n",
    "    instructions = instructions_design()\n",
    "    \n",
    "    \n",
    "    instruction = [instructions['orderOfEntity']][0].split(\"<SEP>\")\n",
    "#     instruction = [instructions['orderOfParagraph']][0].split(\"<SEP>\")\n",
    "#     type_instruction = [instructions['entityType']][0].split(\"<SEP>\")\n",
    "    \n",
    "    paragraph_instruction = '\\n\"\"\"\\n' + paragraph + '\\n\"\"\"\\n'\n",
    "#     type_instruction =  type_instruction[0]+ head_entity + type_instruction[1] + head_type + type_instruction[2]+ tail_entity + type_instruction[3] + tail_type + type_instruction[4]\n",
    "    \n",
    "    concat_instruction = instruction[1]+head_entity+instruction[2]+tail_entity+instruction[3]\n",
    "\n",
    "    \n",
    "    sentence_instruction = [instructions['orderOfParagraph']][0].split(\"<SEP>\")[0] + paragraph_instruction\n",
    "\n",
    "    handcrafted_prompt = sentence_instruction+concat_instruction + '\\n' + instructions['attention'] + rel_in_prompt + '\\n'\n",
    "\n",
    "#     # Basic\n",
    "#     handcrafted_prompt = concat_instruction + '\\n' + rel_in_prompt + '\\n' + paragraph_instruction \n",
    "    prompt = handcrafted_prompt\n",
    "\n",
    "    return prompt\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gpt(my_prompt):\n",
    "    \n",
    "\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=my_prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #   model=\"gpt-3.5-turbo\",\n",
    "    #   messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"You are a helpful assistant to do a relation classification task.\"},\n",
    "    #         {\"role\": \"user\", \"content\": my_prompt},\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    return response\n",
    "\n",
    "def relation_classification(triples, rel_in_prompt_group, paragraph, name_type_dict):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    prediction = 0\n",
    "    for sample in triples:\n",
    "        head_entity = sample[0]\n",
    "        tail_entity = sample[1]\n",
    "        relations = sample[2]\n",
    "        output = ''\n",
    "#         with open(output_file, \"a\") as f:\n",
    "#             f.write('----------------\\n')        \n",
    "        for rel_in_prompt in rel_in_prompt_group:\n",
    "        \n",
    "            prompt = prompt_design(head_entity,tail_entity, rel_in_prompt, paragraph, name_type_dict)\n",
    "            print(\"Prompt==============\")\n",
    "            print(prompt)\n",
    "            print(\"Output==============\")\n",
    "            output_relation = gpt(prompt)[\"choices\"][0][\"text\"]\n",
    "            # output_relation = gpt(prompt)[\"choices\"][0]['message']['content']\n",
    "            print(output_relation)\n",
    "            output+=output_relation + '\\n'\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(\"The relation between \"+ head_entity+ ' and '+ tail_entity+ ' is:')\n",
    "#         print(output)\n",
    "        output = output.lower()\n",
    "        for relation in relations:\n",
    "            if relation in output:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "            \n",
    "     \n",
    "        \n",
    "        for line in output.split(\"\\n\"):\n",
    "            if line != 'none'and line != '' and line!='\\n':\n",
    "                with open(output_file, \"a\") as f:\n",
    "                    f.write('( '+head_entity+\", \"+tail_entity+', '+line+' )\\n')\n",
    "#                     f.write(line+'\\n')\n",
    "                print(\"line === \"+line+\"\\n\")\n",
    "                prediction+=1\n",
    "\n",
    "#         with open(output_file, \"a\") as f:\n",
    "#             f.write('----------------\\n')  \n",
    "        print('-----------TIME-------------------')\n",
    "        localtime = time.localtime()\n",
    "        result = time.strftime(\"%I:%M:%S %p\", localtime)\n",
    "        print(result)\n",
    "        print('-----------TIME-------------------')\n",
    "        \n",
    "#         time.sleep(10)\n",
    "\n",
    "#         print('\\n')\n",
    "\n",
    "    return correct,total, prediction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline prompt\n",
    "\n",
    "# Form the preprocessed dataset.\n",
    "\n",
    "def evaluation(fscore_record, i, correct, c, predictions,ground_truth_samples):\n",
    "    print(\"For first \"+ str(i+1)+ \" documents ==========\\n\")\n",
    "    precision = correct / predictions\n",
    "\n",
    "\n",
    "    recall = correct / ground_truth_samples\n",
    "\n",
    "    print(\"True positive in this round ====\"+str(c))\n",
    "    print(\"True positive in all rounds ====\"+str(correct))\n",
    "    print(\"Number of predictions: ====\"+str(predictions))\n",
    "    print(\"precision ====\"+str(precision))\n",
    "    print(\"ground_truth_samples ====\"+str(ground_truth_samples))\n",
    "    print(\"recall=======\"+ str(recall))\n",
    "\n",
    "    if(predictions == 0 or (precision + recall) == 0):\n",
    "        fscore_record.append('NA')\n",
    "        return \n",
    "\n",
    "    fscore = 2*precision*recall/(precision + recall)\n",
    "    fscore_record.append(fscore)\n",
    "\n",
    "    print(\"The f1-score of the GPT-3 model is : \"+ str(fscore))\n",
    "    \n",
    "    \n",
    "def batch(data):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    ground_truth_samples = 0\n",
    "    predictions = 0\n",
    "    fscore_record = []\n",
    "    print(\"The file contains \"+str(len(data))+\" documents\")\n",
    "    # For all the documents in the dataset\n",
    "    for i in range(len(data)):\n",
    "#         if i ==0 or i == 1 or i == 2:\n",
    "#             continue\n",
    "        article_data = data[i]\n",
    "        triples, rel_in_prompt_group, paragraph, name_type_dict = preprocess(article_data)\n",
    "        print(triples)\n",
    "        c,t,pred = relation_classification(triples, rel_in_prompt_group, paragraph, name_type_dict)\n",
    "        correct += c\n",
    "        total += t\n",
    "        predictions += pred\n",
    "        # ground_truth_samples += len(triples) # It's value acctually eaquals to 't'\n",
    "        ground_truth_samples +=t\n",
    "        \n",
    "\n",
    "        evaluation(fscore_record, i, correct, c, predictions,ground_truth_samples)\n",
    "    \n",
    "        if i == 4:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_selection(data):\n",
    "    sorted_data = sorted(data, key=lambda x: len(x['labels']))\n",
    "    return sorted_data\n",
    "    \n",
    "    \n",
    "    \n",
    "def choose_documents(data,choose):\n",
    "    \n",
    "    document100 = data[1:101]\n",
    "\n",
    "    for i in range(len(document100)):\n",
    "        if i in choose:\n",
    "            infor = 'This is document '+str(i+1)+': '+document100[i]['title']+'\\n'\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(\"========================\\n\")\n",
    "                f.write(infor)\n",
    "            document = document100[i]\n",
    "            triples, rel_in_prompt_group, paragraph, name_type_dict = preprocess(document)\n",
    "\n",
    "            c,t,pred = relation_classification(triples, rel_in_prompt_group, paragraph, name_type_dict)\n",
    "            print(\"True Postive ===========\"+str(c))\n",
    "            print(\"Total triples===========\"+str(t))\n",
    "            print(\"TP + FN===========\"+str(pred))\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(\"TP = \"+ str(c)+\"\\n\")        \n",
    "                f.write(\"Total = \"+ str(t)+\"\\n\")        \n",
    "                f.write(\"Predictions = \"+ str(pred)+\"\\n\")        \n",
    "                f.write(\"=======================\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "choose_documents(document_selection(data), choosed_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Prompt:\n",
    "\n",
    "Instruction: the paragraph is from an article of Wikipedia.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Matthew de Glendonwyn ( died 10 May 1408 ) was a late 14th and early 15th century bishop of Glasgow . He was elected to the see after the death of Cardinal Walter Wardlaw , his predecessor as bishop . He was elected sometime between Wardlaw 's death in September and Matthew 's first appearance as bishop - elect in December . Matthew was consecrated some months after his election , either in late 1387 or early 1388 . Matthew was a witness to charters of kings Robert II and Robert III , an occasional ambassador of the Scottish crown to England , and a frequent arbiter in disputes concerning various religious establishments . On 21 May 1401 , he introduced a tax in his diocese to improve the deficient ornamenta of the diocese ( i.e. chasubles , copes , dalmatics , etc . ) . According to the Martyrology of Glasgow , he died on 10 May 1408 .\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "For entity \"Matthew\", what relationship is entity \"10 May 1408\" to it?\n",
    "\n",
    "Attention: You are restricted to choose from the following relationships and if there are matching relationships below, please output the relations directly, otherwise output \"NONE\":\n",
    "- genre\n",
    "- mother\n",
    "- applies to jurisdiction\n",
    "- father\n",
    "- present in work\n",
    "- religion\n",
    "- date of death\n",
    "- work location\n",
    "- territory claimed by\n",
    "- award received\n",
    "- continent\n",
    "- location of formation\n",
    "- head of government\n",
    "- parent organization\n",
    "- official language\n",
    "- creator\n",
    "- legislative body\n",
    "- cast member\n",
    "- dissolved, abolished or demolished\n",
    "- composer\n",
    "- developer\n",
    "- capital of\n",
    "- notable work\n",
    "- sister city\n",
    "- position held\n",
    "- original language of work\n",
    "- basin country\n",
    "- production company\n",
    "- mouth of the watercourse\n",
    "- point in time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
